{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ae6c59",
   "metadata": {},
   "source": [
    "# Running Inference Locally\n",
    "\n",
    "Large Language Models (LLMs) have revolutionized AI applications, but they don't always need to be accessed through cloud APIs. In this lesson, we'll explore how to download, save, and run LLMs locally in your development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac9f0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\tejes\\desktop\\manali\\.venv\\lib\\site-packages (from typer-slim->transformers) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries (if not already installed)\n",
    "!pip install transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a6fe5",
   "metadata": {},
   "source": [
    "Understanding Local LLMs\n",
    "\n",
    "Running LLMs locally offers several advantages:\n",
    "- **Privacy**: Your data doesn't leave your environment\n",
    "- **Cost**: No per-token API charges\n",
    "- **Latency**: No network delays\n",
    "- **Customization**: Full control over model parameters\n",
    "\n",
    "However, local LLMs also have limitations:\n",
    "- **Hardware requirements**: Models need sufficient RAM and GPU\n",
    "- **Model size**: Smaller models fit locally but may have reduced capabilities\n",
    "- **Updates**: You manage model versions yourself\n",
    "\n",
    "Let's start by downloading a small LLM called DistilGPT2, a distilled version of GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae5057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tejes\\Desktop\\Manali\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tejes\\Desktop\\Manali\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tejes\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 76/76 [00:00<00:00, 593.62it/s, Materializing param=transformer.wte.weight]            \n",
      "GPT2LMHeadModel LOAD REPORT from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: distilgpt2\n",
      "Number of parameters: 81,912,576\n",
      "Model size on disk: ~312.47 MB (estimated)\n",
      "\n",
      "Saving model to ./downloaded_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Set the directory where you want to save the model\n",
    "save_directory = \"./downloaded_model\"  # Change this to your preferred path\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Downloading model from Hugging Face Hub...\")\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Print model information\n",
    "print(f\"\\nModel: {model_name}\")\n",
    "print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "print(\n",
    "    f\"Model size on disk: ~{model.num_parameters() * 4 / (1024 * 1024):.2f} MB (estimated)\")\n",
    "\n",
    "# Save the model and tokenizer to the specified directory\n",
    "print(f\"\\nSaving model to {save_directory}...\")\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5d342",
   "metadata": {},
   "source": [
    "## Loading and Using a Local Model\n",
    "\n",
    "Once saved, we can load the model from local storage instead of downloading it again. This is especially useful for larger models or when working in environments with limited internet access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c411dc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 76/76 [00:00<00:00, 686.99it/s, Materializing param=transformer.wte.weight]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from local directory!\n",
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Now we can load from local directory instead of downloading again\n",
    "print(\"Loading model from local directory...\")\n",
    "local_model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "local_tokenizer.pad_token = local_tokenizer.eos_token\n",
    "print(\"Model loaded from local directory!\")\n",
    "\n",
    "# Codespace is mostly run on cpus, so we're going to use a CPU\n",
    "device = \"cpu\"\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ac5b2",
   "metadata": {},
   "source": [
    "## Generating Text with Your Local LLM\n",
    "\n",
    "Now let's create a more versatile text generation function that allows us to control various parameters:\n",
    "\n",
    "- **Temperature**: Controls randomness (higher = more creative, lower = more deterministic)\n",
    "- **Max length**: The maximum number of tokens to generate\n",
    "- **Top-p (nucleus sampling)**: Limits token selection to a subset of most likely tokens\n",
    "- **Top-k**: Limits selection to the k most likely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe6b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt,\n",
    "                  max_length=50,\n",
    "                  temperature=0.8,\n",
    "                  top_p=0.9,\n",
    "                  top_k=50,\n",
    "                  do_sample=True):\n",
    "    \"\"\"Generate text from a prompt with customizable parameters\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input text to continue\n",
    "        max_length (int): Maximum length of generated text (including prompt)\n",
    "        temperature (float): Higher values (>1.0) increase randomness, lower values (<1.0) make it more deterministic\n",
    "        top_p (float): Nucleus sampling parameter (0-1.0)\n",
    "        top_k (int): Limits selection to k most likely tokens\n",
    "        do_sample (bool): If False, uses greedy decoding instead of sampling\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated text including the prompt\n",
    "    \"\"\"\n",
    "    # Prepare the inputs\n",
    "    inputs = local_tokenizer(prompt, return_tensors=\"pt\",\n",
    "                             return_attention_mask=True).to(device)\n",
    "\n",
    "    # Generate text\n",
    "    output = local_model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        pad_token_id=local_tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = local_tokenizer.decode(\n",
    "        output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up excess whitespace with regex\n",
    "    import re\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', generated_text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c9b3f",
   "metadata": {},
   "source": [
    "## Experimenting with Different Generation Parameters\n",
    "\n",
    "Let's try generating text with different parameters to see how they affect the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d410e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Default parameters (temperature=0.8)\n",
      "Prompt: \"Welcome to Fundamentals of AI Engineering on LinkedIn Learning. This class\"\n",
      "Generated: Welcome to Fundamentals of AI Engineering on LinkedIn Learning. This class focuses on the topic of AI engineering, how to build an AI-powered business that makes AI the world's best in all ways. We're also looking to learn more about the role AI is playing in the business. \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 2: Low temperature (more deterministic)\n",
      "Prompt: \"Welcome to Fundamentals of AI Engineering on LinkedIn Learning. This class\"\n",
      "Generated: Welcome to Fundamentals of AI Engineering on LinkedIn Learning. This class is designed to help you learn the basics of AI engineering. \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 3: High temperature (more creative/random)\n",
      "Prompt: \"Welcome to Fundamentals of AI Engineering on LinkedIn Learning. This class\"\n",
      "Generated: Welcome to Fundamentals of AI Engineering on LinkedIn Learning. This class includes all four types of skills and will involve starting with your best work of our engineering class to begin with and after graduate students in general have completed their engineering education in AI before making a formal certification for advanced AI technology at UC Davis! In terms of my major technical background (and an\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Example 4: Greedy decoding (no sampling, always selects most likely token)\n",
      "Prompt: \"Welcome to Fundamentals of AI Engineering on LinkedIn Learning. This class\"\n",
      "Generated: Welcome to Fundamentals of AI Engineering on LinkedIn Learning. This class is free and open to all. \n"
     ]
    }
   ],
   "source": [
    "prompt = \"Welcome to Fundamentals of AI Engineering on LinkedIn Learning. This class\"\n",
    "\n",
    "print(\"Example 1: Default parameters (temperature=0.8)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, max_length=75)}\")\n",
    "print(\"-\"*100 + \"\\n\")\n",
    "print(\"Example 2: Low temperature (more deterministic)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, temperature=0.2, max_length=75)}\")\n",
    "print(\"-\"*100 + \"\\n\")\n",
    "print(\"Example 3: High temperature (more creative/random)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, temperature=1.5, max_length=75)}\")\n",
    "print(\"-\"*100 + \"\\n\")\n",
    "print(\"Example 4: Greedy decoding (no sampling, always selects most likely token)\")\n",
    "print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "print(f\"Generated: {generate_text(prompt, top_p=None, temperature=None, do_sample=False, max_length=75)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd59bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
